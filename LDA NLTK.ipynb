{
 "metadata": {
  "name": "",
  "signature": "sha256:dae8d5d41ada37383340adda813c59852eee32b9ebb9e36d2c6e9d3c08508fde"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### M\u00e9todos para pre processamento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import sys\n",
      "import os\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from nltk.corpus import machado as mch\n",
      "import nltk\n",
      "from gensim import corpora, models, similarities\n",
      "from gensim.models import hdpmodel, ldamodel\n",
      "from   Topics.visualization.wordcloud import make_wordcloud\n",
      "from IPython.display import display, Image\n",
      "import glob\n",
      "\n",
      "\n",
      "import logging as log\n",
      "\n",
      "FONT_PATH = 'C:\\Windows\\Fonts\\Arial.ttf'\n",
      "\n",
      "\n",
      "logFormatter = log.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
      "rootLogger = log.getLogger()\n",
      "\n",
      "fileHandler = log.FileHandler(\"{0}/{1}.log\".format('./', 'log.txt'))\n",
      "fileHandler.setFormatter(logFormatter)\n",
      "rootLogger.addHandler(fileHandler)\n",
      "\n",
      "consoleHandler = log.StreamHandler()\n",
      "consoleHandler.setFormatter(logFormatter)\n",
      "rootLogger.addHandler(consoleHandler)\n",
      "\n",
      "\n",
      "reload(sys)\n",
      "sys.setdefaultencoding(\"utf-8\")\n",
      "\n",
      "def nomes_documentos(pasta):\n",
      "    return os.listdir(pasta+'/tokens')\n",
      "\n",
      "def stopwords_en():\n",
      "    f = open('stopwords_en.txt','r')\n",
      "    stopw = f.read().split('\\n')\n",
      "    f.close()\n",
      "    return stopw\n",
      "\n",
      "\n",
      "def stopwords_pt():\n",
      "    f = open('stopwords_pt.txt','r')\n",
      "    stopw = f.read().split('\\n')\n",
      "    f.close()\n",
      "    return stopw\n",
      "\n",
      "def cria_pastas(pasta):\n",
      "    if not os.path.exists(pasta):\n",
      "        os.makedirs(pasta)\n",
      "    dirs = [pasta+'/tokens',pasta+'/freq',pasta+'/vocab']\n",
      "    for d in dirs:\n",
      "        if not os.path.exists(d):\n",
      "            os.makedirs(d)\n",
      "\n",
      "def pre_processamento(pasta, nltk_corpus, stopwords, min_rep=10):\n",
      "    \n",
      "    for fileid in nltk_corpus.fileids():\n",
      "        t1 = datetime.datetime.now()\n",
      "        print 'Tokenizar %s...' % fileid \n",
      "        \n",
      "        # Carrega todas as palavras.\n",
      "        words = nltk_corpus.words(fileid)\n",
      "        \n",
      "        # Filtra palavras menores que 3 caracteres e stopwords.\n",
      "        doc = [w.lower() for w in words if len(w) > 3 and  w.lower() not in stopwords]\n",
      "\n",
      "        # Filtra palavras que aparecem menos de 10 vezes.\n",
      "        fd = nltk.FreqDist(doc)\n",
      "        if min_rep > 0 :\n",
      "            doc = [w for w in doc if fd[w] > min_rep]\n",
      "       \n",
      "            if len(doc) == 0:\n",
      "                raise Exception ('Filtro com {} palaras repetidas diminuou bastatnte o vocabulario.'.format(min_rep))\n",
      "\n",
      "        # Salva palavras em um novo documentos.\n",
      "        f = open(pasta+'/tokens/'+fileid.replace('/','_'),'w')\n",
      "        for w in doc:\n",
      "            f.write(w+u'\\n')\n",
      "        f.close()\n",
      "        delta_t = datetime.datetime.now()-t1\n",
      "        print 'Tokenizado  ''%s'' em %d segundos.'%(fileid, delta_t.seconds) \n",
      "\n",
      "        \n",
      "def registra_freq(pasta):\n",
      "    for i in nomes_documentos(pasta):\n",
      "        fdoc = open(pasta+'/tokens/' + i,'r')\n",
      "        doc = fdoc.read().split('\\n')\n",
      "        fdoc.close()\n",
      "        fd = nltk.FreqDist(doc)\n",
      "        ffreq = open(pasta+'/freq/'+i, 'w')\n",
      "        for k,v in fd.items():\n",
      "            ffreq.write(k+';'+str(v)+'\\n')\n",
      "        ffreq.close()\n",
      "        \n",
      "        \n",
      "\n",
      "def constroi_vocabulario(pasta):\n",
      "    items = None\n",
      "    vocab = set([])\n",
      "    palavras = []\n",
      "    for arqv in os.listdir(pasta+'/freq'):\n",
      "        f = open(pasta+'/freq/'+arqv, 'r')\n",
      "        items = f.read().split('\\n')\n",
      "        f.close()\n",
      "        lista = []\n",
      "        for i in items:\n",
      "            word = i.split(';')[0]\n",
      "            lista.append(word)\n",
      "            palavras.append(word)\n",
      "        vocab = set(list(vocab)+lista)\n",
      "    f = open(pasta+'/vocab/vocab.txt','w')\n",
      "    txt = '\\n'.join(sorted(vocab))\n",
      "    f.write(txt)\n",
      "    f.close()\n",
      "\n",
      "    # Registra a ocorr\u00eancia das palavras entre os documentos.\n",
      "    fd = nltk.FreqDist(palavras)\n",
      "    ffreq = open(pasta+'/palavras_comuns.txt','w')\n",
      "    for k,v in fd.items():\n",
      "        ffreq.write(k+';'+str(v)+'\\n')\n",
      "    ffreq.close()\n",
      "\n",
      "def palavras_comuns(pasta):    \n",
      "    f = open(pasta+'/palavras_comuns.txt', 'r')\n",
      "    txt = f.read().splitlines()\n",
      "    f.close()\n",
      "    return txt\n",
      "\n",
      "\n",
      "def vocabulario():\n",
      "    f = open(pasta+'/vocab/vocab.txt', 'r')\n",
      "    txt = f.read().splitlines()\n",
      "    f.close()\n",
      "    return txt\n",
      "\n",
      "def documentos():\n",
      "    arqvs = os.listdir(pasta+'/tokens')\n",
      "    docs = []\n",
      "    for arq in arqvs:\n",
      "        f = open(pasta+'/tokens/'+arq)\n",
      "        words = f.read().splitlines()\n",
      "        #words = ' '.join(words)\n",
      "        docs.append(words)\n",
      "        f.close()\n",
      "    return docs\n",
      "\n",
      "\n",
      "def lda_flavio(num_topics=10,pasta='./'):\n",
      "    vocab = vocabulario()\n",
      "    docset = documentos()\n",
      "    \n",
      "    K=num_topics\n",
      "    D = len(docset) #Number of documents in the docset\n",
      "    olda = onlineldavb.OnlineLDA(vocab, K, D, 1./K, 1./K, 1024, 0.7)\n",
      "    i = 0\n",
      "    for doc in docset:\n",
      "        t1 = datetime.datetime.now()\n",
      "        i += 1\n",
      "        print 'Analistando documento %d de %d.' % (i, D)\n",
      "        gamma, bound = olda.update_lambda(doc)\n",
      "        wordids, wordcts = onlineldavb.parse_doc_list(doc,olda._vocab)\n",
      "        perwordbound = bound * len(docset) / (D*sum(map(sum,wordcts)))\n",
      "        delta_t = datetime.datetime.now()-t1\n",
      "        print 'Documento  %d de %d em %d segundos.'%(i, D, delta_t.seconds) #2134 segundos ~ 35 min.\n",
      "\n",
      "    np.savetxt(pasta+'lda_lambda.txt',olda._lambda)\n",
      "    print 'Fim do LDA'\n",
      "    return vocab, olda._lambda\n",
      "\n",
      "\n",
      "def lda_gensim(pasta, num_topics):\n",
      "    texts = documentos()\n",
      "    \n",
      "    dictionary = corpora.Dictionary(texts)\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "    \n",
      "    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
      "    \n",
      "    lda.save(pasta+'/lda')\n",
      "    dictionary.save(pasta+'/dic')\n",
      "    dictionary.save_as_text(pasta+'/dic.txt')  \n",
      "    \n",
      "    \n",
      "    return (dictionary,corpus,lda)\n",
      "\n",
      "def gensim_extract(lda, n_topics=10, n_words=10):\n",
      "    p = []\n",
      "    for top in lda.show_topics(topics=n_topics, topn=n_words):\n",
      "        lista_p = top.split(' + ') \n",
      "        vk = [(l.split('*')[1],float(l.split('*')[0])) for l in lista_p ]\n",
      "        p.append(vk)\n",
      "        #[l[1] for l in lista_v]\n",
      "    return p\n",
      "\n",
      "\n",
      "\n",
      "def gera_wordclouds(lda, pasta='./', n_topics=10, n_words=10):\n",
      "    i = 0\n",
      "    html = '<html><body>\\n'\n",
      "    \n",
      "    for topico in gensim_extract(lda, n_topics=n_topics, n_words=n_words):\n",
      "        i += 1\n",
      "        words = []\n",
      "        prob = []\n",
      "        for item in topico:\n",
      "            words.append(item[0])\n",
      "            prob.append(item[1])\n",
      "        img = 'topico_{:02}.png'.format(i)\n",
      "        make_wordcloud(np.array(words), np.array(prob), fname=\"{}/{}\".format(pasta, img), font_path=FONT_PATH, width=600, height=300)    \n",
      "        html += '<div><img src=\"'+img+'\" style=\"margin:10px\"/></div>\\n'\n",
      "\n",
      "    html += '</body></html>'\n",
      "\n",
      "    fname = pasta+'/topicos.html'\n",
      "  \n",
      "    f = open(fname, 'w')\n",
      "    f.write(html)\n",
      "    f.close()\n",
      "\n",
      "def obtem_topicos(lda, pasta='.', n_topics=10, n_words=10):\n",
      "    topics = []\n",
      "    dist = []\n",
      "    for t in gensim_extract(lda, n_topics=n_topics, n_words=n_words):\n",
      "        words = []\n",
      "        prob = []\n",
      "        for item in t:\n",
      "            words.append(item[0])\n",
      "            prob.append(item[1])\n",
      "        topics.append(words)\n",
      "        dist.append(prob)\n",
      "    return topics,dist\n",
      "\n",
      "    \n",
      "def exibe_wordclouds_inline(pasta='.'):\n",
      "    for img in glob.glob(pasta+'/*.png'):\n",
      "        display(Image(filename=img))\n",
      "\n",
      "def topicos_txt(lda, pasta='.', n_topics=10, n_words=10):       \n",
      "    top,dist = obtem_topicos( lda, pasta=pasta, n_topics=n_topics, n_words=n_words)\n",
      "    t = PrettyTable()\n",
      "    t.padding_width = 1\n",
      "    for i in range(len(top)):\n",
      "        t.add_column('Topic{}'.format(i), top[i])\n",
      "\n",
      "    f = open(pasta+'/topicos.txt','w')\n",
      "    f.write(t.get_string())\n",
      "    f.close()\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Corpus NLTK."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pasta = 'machado'\n",
      "corpus = nltk.corpus.machado\n",
      "stopwords = stopwords_pt()\n",
      "min_rep = -1\n",
      "num_topics=10\n",
      "num_words = 20\n",
      "#cria_pastas(pasta)\n",
      "#pre_processamento(pasta, corpus, stopwords,min_rep=min_rep)\n",
      "#registra_freq(pasta)\n",
      "#constroi_vocabulario(pasta)\n",
      "dic,corpus, lda = lda_gensim(pasta, num_topics=num_topics)\n",
      "#gera_wordclouds(lda, pasta=pasta,n_topics=lda.num_topics, n_words=num_words)\n",
      "topicos_txt(lda, pasta, n_topics=lda.num_topics, n_words=num_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pasta = 'movie_reviews'\n",
      "corpus = nltk.corpus.movie_reviews\n",
      "stopwords = stopwords_en()\n",
      "min_rep = -1\n",
      "num_topics=10\n",
      "num_words = 20\n",
      "#cria_pastas(pasta)\n",
      "#pre_processamento(pasta, corpus, stopwords,min_rep=min_rep)\n",
      "#registra_freq(pasta)\n",
      "#constroi_vocabulario(pasta)\n",
      "dic,corpus, lda = lda_gensim(pasta, num_topics=num_topics)\n",
      "#gera_wordclouds(lda, pasta=pasta,n_topics=lda.num_topics, n_words=num_words)\n",
      "topicos_txt(lda, pasta, n_topics=lda.num_topics, n_words=num_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pasta = 'mac_morpho'\n",
      "corpus = nltk.corpus.mac_morpho\n",
      "stopwords = stopwords_pt()\n",
      "min_rep = -1\n",
      "num_topics=10\n",
      "num_words = 20\n",
      "#cria_pastas(pasta)\n",
      "#pre_processamento(pasta, corpus, stopwords,min_rep=min_rep)\n",
      "#registra_freq(pasta)\n",
      "#constroi_vocabulario(pasta)\n",
      "dic,corpus, lda = lda_gensim(pasta, num_topics=num_topics)\n",
      "#gera_wordclouds(lda, pasta=pasta,n_topics=lda.num_topics, n_words=num_words)\n",
      "topicos_txt(lda, pasta, n_topics=lda.num_topics, n_words=num_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Carregar LDA feito anteriomente"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = ldamodel.LdaModel.load('machado/lda')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clear%"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}