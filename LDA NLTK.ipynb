{
 "metadata": {
  "name": "",
  "signature": "sha256:7f433e104875c1d1fd9e69d28a5a658660f4818d5c4a1f00b9063f80873cc2b0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### M\u00e9todos para pre processamento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import sys\n",
      "import os\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from nltk.corpus import machado as mch\n",
      "import nltk\n",
      "from gensim import corpora, models, similarities\n",
      "from gensim.models import hdpmodel, ldamodel\n",
      "from   Topics.visualization.wordcloud import make_wordcloud\n",
      "from IPython.display import display, Image\n",
      "import glob\n",
      "from prettytable import PrettyTable\n",
      "\n",
      "\n",
      "import logging as log\n",
      "\n",
      "FONT_PATH = 'C:\\Windows\\Fonts\\Arial.ttf'\n",
      "\n",
      "\n",
      "logFormatter = log.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
      "rootLogger = log.getLogger()\n",
      "\n",
      "fileHandler = log.FileHandler(\"{0}/{1}.log\".format('./', 'log.txt'))\n",
      "fileHandler.setFormatter(logFormatter)\n",
      "rootLogger.addHandler(fileHandler)\n",
      "\n",
      "consoleHandler = log.StreamHandler()\n",
      "consoleHandler.setFormatter(logFormatter)\n",
      "rootLogger.addHandler(consoleHandler)\n",
      "\n",
      "\n",
      "reload(sys)\n",
      "sys.setdefaultencoding(\"utf-8\")\n",
      "\n",
      "def nomes_documentos(pasta):\n",
      "    return os.listdir(pasta+'/tokens')\n",
      "\n",
      "def stopwords_en():\n",
      "    f = open('stopwords_en.txt','r')\n",
      "    stopw = f.read().split('\\n')\n",
      "    f.close()\n",
      "    return stopw\n",
      "\n",
      "\n",
      "def stopwords_pt():\n",
      "    f = open('stopwords_pt.txt','r')\n",
      "    stopw = f.read().split('\\n')\n",
      "    f.close()\n",
      "    return stopw\n",
      "\n",
      "def cria_pastas(pasta):\n",
      "    if not os.path.exists(pasta):\n",
      "        os.makedirs(pasta)\n",
      "    dirs = [pasta+'/tokens',pasta+'/freq',pasta+'/vocab']\n",
      "    for d in dirs:\n",
      "        if not os.path.exists(d):\n",
      "            os.makedirs(d)\n",
      "\n",
      "def pre_processamento(pasta, nltk_corpus, stopwords, min_rep=10):\n",
      "    \n",
      "    for fileid in nltk_corpus.fileids():\n",
      "        t1 = datetime.datetime.now()\n",
      "        print 'Tokenizar %s...' % fileid \n",
      "        \n",
      "        # Carrega todas as palavras.\n",
      "        words = nltk_corpus.words(fileid)\n",
      "        \n",
      "        # Filtra palavras menores que 3 caracteres e stopwords.\n",
      "        doc = [w.lower() for w in words if len(w) > 3 and  w.lower() not in stopwords]\n",
      "\n",
      "        # Filtra palavras que aparecem menos de 10 vezes.\n",
      "        fd = nltk.FreqDist(doc)\n",
      "        if min_rep > 0 :\n",
      "            doc = [w for w in doc if fd[w] > min_rep]\n",
      "       \n",
      "            if len(doc) == 0:\n",
      "                raise Exception ('Filtro com {} palaras repetidas diminuou bastatnte o vocabulario.'.format(min_rep))\n",
      "\n",
      "        # Salva palavras em um novo documentos.\n",
      "        f = open(pasta+'/tokens/'+fileid.replace('/','_'),'w')\n",
      "        for w in doc:\n",
      "            f.write(w+u'\\n')\n",
      "        f.close()\n",
      "        delta_t = datetime.datetime.now()-t1\n",
      "        print 'Tokenizado  ''%s'' em %d segundos.'%(fileid, delta_t.seconds) \n",
      "\n",
      "        \n",
      "def registra_freq(pasta):\n",
      "    for i in nomes_documentos(pasta):\n",
      "        fdoc = open(pasta+'/tokens/' + i,'r')\n",
      "        doc = fdoc.read().split('\\n')\n",
      "        fdoc.close()\n",
      "        fd = nltk.FreqDist(doc)\n",
      "        ffreq = open(pasta+'/freq/'+i, 'w')\n",
      "        for k,v in fd.items():\n",
      "            ffreq.write(k+';'+str(v)+'\\n')\n",
      "        ffreq.close()\n",
      "        \n",
      "        \n",
      "\n",
      "def constroi_vocabulario(pasta):\n",
      "    items = None\n",
      "    vocab = set([])\n",
      "    palavras = []\n",
      "    for arqv in os.listdir(pasta+'/freq'):\n",
      "        f = open(pasta+'/freq/'+arqv, 'r')\n",
      "        items = f.read().split('\\n')\n",
      "        f.close()\n",
      "        lista = []\n",
      "        for i in items:\n",
      "            word = i.split(';')[0]\n",
      "            lista.append(word)\n",
      "            palavras.append(word)\n",
      "        vocab = set(list(vocab)+lista)\n",
      "    f = open(pasta+'/vocab/vocab.txt','w')\n",
      "    txt = '\\n'.join(sorted(vocab))\n",
      "    f.write(txt)\n",
      "    f.close()\n",
      "\n",
      "    # Registra a ocorr\u00eancia das palavras entre os documentos.\n",
      "    fd = nltk.FreqDist(palavras)\n",
      "    ffreq = open(pasta+'/palavras_comuns.txt','w')\n",
      "    for k,v in fd.items():\n",
      "        ffreq.write(k+';'+str(v)+'\\n')\n",
      "    ffreq.close()\n",
      "\n",
      "def palavras_comuns(pasta):    \n",
      "    f = open(pasta+'/palavras_comuns.txt', 'r')\n",
      "    txt = f.read().splitlines()\n",
      "    f.close()\n",
      "    return txt\n",
      "\n",
      "\n",
      "def vocabulario():\n",
      "    f = open(pasta+'/vocab/vocab.txt', 'r')\n",
      "    txt = f.read().splitlines()\n",
      "    f.close()\n",
      "    return txt\n",
      "\n",
      "def documentos(pasta, filtro=''):\n",
      "    #arqvs = os.listdir(pasta+'/tokens')\n",
      "    if filtro != '': filtro = '/' + filtro\n",
      "    arqvs = glob.glob(pasta+'/tokens'+filtro)\n",
      "    docs = []\n",
      "    for arq in arqvs:\n",
      "        f = open(arq)\n",
      "        words = f.read().splitlines()\n",
      "        #words = ' '.join(words)\n",
      "        docs.append(words)\n",
      "        f.close()\n",
      "    return docs\n",
      "\n",
      "\n",
      "def lda_flavio(num_topics=10,pasta='./'):\n",
      "    vocab = vocabulario()\n",
      "    docset = documentos()\n",
      "    \n",
      "    K=num_topics\n",
      "    D = len(docset) #Number of documents in the docset\n",
      "    olda = onlineldavb.OnlineLDA(vocab, K, D, 1./K, 1./K, 1024, 0.7)\n",
      "    i = 0\n",
      "    for doc in docset:\n",
      "        t1 = datetime.datetime.now()\n",
      "        i += 1\n",
      "        print 'Analistando documento %d de %d.' % (i, D)\n",
      "        gamma, bound = olda.update_lambda(doc)\n",
      "        wordids, wordcts = onlineldavb.parse_doc_list(doc,olda._vocab)\n",
      "        perwordbound = bound * len(docset) / (D*sum(map(sum,wordcts)))\n",
      "        delta_t = datetime.datetime.now()-t1\n",
      "        print 'Documento  %d de %d em %d segundos.'%(i, D, delta_t.seconds) #2134 segundos ~ 35 min.\n",
      "\n",
      "    np.savetxt(pasta+'lda_lambda.txt',olda._lambda)\n",
      "    print 'Fim do LDA'\n",
      "    return vocab, olda._lambda\n",
      "\n",
      "\n",
      "def lda_gensim(pasta, num_topics,filtro=''):\n",
      "    texts = documentos(pasta,filtro)\n",
      "    \n",
      "    dictionary = corpora.Dictionary(texts)\n",
      "    #dictionary = corpora.Dictionary()\n",
      "    \n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "    \n",
      "    lda = ldamodel.LdaModel([corpus[0]], id2word=dictionary, num_topics=num_topics )\n",
      "    \n",
      "    for i in range(1,len(corpus)):\n",
      "        lda.update([corpus[i]])\n",
      "    \n",
      "    lda.save(pasta+'/lda')\n",
      "    dictionary.save(pasta+'/dic')\n",
      "    dictionary.save_as_text(pasta+'/dic.txt')  \n",
      "    \n",
      "    \n",
      "    return (dictionary,corpus,lda)\n",
      "\n",
      "def gensim_extract(lda, n_topics=10, n_words=10):\n",
      "    p = []\n",
      "    for top in lda.show_topics(topics=n_topics, topn=n_words):\n",
      "        lista_p = top.split(' + ') \n",
      "        vk = [(l.split('*')[1],float(l.split('*')[0])) for l in lista_p ]\n",
      "        p.append(vk)\n",
      "        #[l[1] for l in lista_v]\n",
      "    return p\n",
      "\n",
      "\n",
      "\n",
      "def gera_wordclouds(lda, pasta='./', n_topics=10, n_words=10):\n",
      "    i = 0\n",
      "    html = '<html><body>\\n'\n",
      "    \n",
      "    for topico in gensim_extract(lda, n_topics=n_topics, n_words=n_words):\n",
      "        i += 1\n",
      "        words = []\n",
      "        prob = []\n",
      "        for item in topico:\n",
      "            words.append(item[0])\n",
      "            prob.append(item[1])\n",
      "        img = 'topico_{:02}.png'.format(i)\n",
      "        make_wordcloud(np.array(words), np.array(prob), fname=\"{}/{}\".format(pasta, img), font_path=FONT_PATH, width=600, height=300)    \n",
      "        html += '<div><img src=\"'+img+'\" style=\"margin:10px\"/></div>\\n'\n",
      "\n",
      "    html += '</body></html>'\n",
      "\n",
      "    fname = pasta+'/topicos.html'\n",
      "  \n",
      "    f = open(fname, 'w')\n",
      "    f.write(html)\n",
      "    f.close()\n",
      "\n",
      "def obtem_topicos(lda, pasta='.', n_topics=10, n_words=10):\n",
      "    topics = []\n",
      "    dist = []\n",
      "    for t in gensim_extract(lda, n_topics=n_topics, n_words=n_words):\n",
      "        words = []\n",
      "        prob = []\n",
      "        for item in t:\n",
      "            words.append(item[0])\n",
      "            prob.append(item[1])\n",
      "        topics.append(words)\n",
      "        dist.append(prob)\n",
      "    return topics,dist\n",
      "\n",
      "    \n",
      "def exibe_wordclouds_inline(pasta='.'):\n",
      "    for img in glob.glob(pasta+'/*.png'):\n",
      "        display(Image(filename=img))\n",
      "\n",
      "def topicos_txt(lda, pasta='.', n_topics=10, n_words=10):       \n",
      "    top,dist = obtem_topicos( lda, pasta=pasta, n_topics=n_topics, n_words=n_words)\n",
      "    t = PrettyTable()\n",
      "    t.padding_width = 1\n",
      "    for i in range(len(top)):\n",
      "        t.add_column('Topic{}'.format(i), top[i])\n",
      "\n",
      "    f = open(pasta+'/topicos.txt','w')\n",
      "    f.write(t.get_string())\n",
      "    f.close()\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Corpus NLTK."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pasta = 'machado'\n",
      "corpus = nltk.corpus.machado\n",
      "stopwords = stopwords_pt()\n",
      "min_rep = -1\n",
      "num_topics=10\n",
      "num_words = 20\n",
      "#cria_pastas(pasta)\n",
      "#pre_processamento(pasta, corpus, stopwords,min_rep=min_rep)\n",
      "#registra_freq(pasta)\n",
      "#constroi_vocabulario(pasta)\n",
      "dic,corpus, lda = lda_gensim(pasta, num_topics=num_topics,filtro='romanc*')\n",
      "#gera_wordclouds(lda, pasta=pasta,n_topics=lda.num_topics, n_words=num_words)\n",
      "topicos_txt(lda, pasta, n_topics=lda.num_topics, n_words=num_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pasta = 'movie_reviews'\n",
      "corpus = nltk.corpus.movie_reviews\n",
      "stopwords = stopwords_en()\n",
      "min_rep = -1\n",
      "num_topics=10\n",
      "num_words = 20\n",
      "#cria_pastas(pasta)\n",
      "#pre_processamento(pasta, corpus, stopwords,min_rep=min_rep)\n",
      "#registra_freq(pasta)\n",
      "#constroi_vocabulario(pasta)\n",
      "dic,corpus, lda = lda_gensim(pasta, num_topics=num_topics)\n",
      "#gera_wordclouds(lda, pasta=pasta,n_topics=lda.num_topics, n_words=num_words)\n",
      "topicos_txt(lda, pasta, n_topics=lda.num_topics, n_words=num_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pasta = 'mac_morpho'\n",
      "corpus = nltk.corpus.mac_morpho\n",
      "stopwords = stopwords_pt()\n",
      "min_rep = -1\n",
      "num_topics=10\n",
      "num_words = 20\n",
      "#cria_pastas(pasta)\n",
      "#pre_processamento(pasta, corpus, stopwords,min_rep=min_rep)\n",
      "#registra_freq(pasta)\n",
      "#constroi_vocabulario(pasta)\n",
      "dic,corpus, lda = lda_gensim(pasta, num_topics=num_topics)\n",
      "#gera_wordclouds(lda, pasta=pasta,n_topics=lda.num_topics, n_words=num_words)\n",
      "topicos_txt(lda, pasta, n_topics=lda.num_topics, n_words=num_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Carregar LDA feito anteriomente"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = ldamodel.LdaModel.load('machado/lda')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda.show_topics()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "['0.050*cabe\\xc3\\xa7a + 0.034*helena + 0.026*palavra + 0.025*gilliatt + 0.013*em\\xc3\\xadlia + 0.013*cec\\xc3\\xadlia + 0.010*costa + 0.009*lethierry + 0.008*leonor + 0.008*tito',\n",
        " '0.038*casa + 0.033*tempo + 0.032*olhos + 0.031*homem + 0.029*coisa + 0.024*dois + 0.022*duas + 0.022*dizer + 0.022*havia + 0.021*todos',\n",
        " '0.033*pedro + 0.028*mesma + 0.023*senhor + 0.020*oliver + 0.018*janela + 0.016*estela + 0.014*grande + 0.013*comigo + 0.012*muita + 0.012*digo',\n",
        " '0.030*nome + 0.030*quatro + 0.024*gilliatt + 0.018*mesa + 0.017*pequeno + 0.015*rubi\\xc3\\xa3o + 0.015*valentim + 0.014*\\xc3\\xa1gua + 0.014*jos\\xc3\\xa9 + 0.013*elisa',\n",
        " '0.019*\\xc3\\xa1gua + 0.018*gilliatt + 0.018*vento + 0.017*livro + 0.015*alves + 0.011*padre + 0.009*paulo + 0.008*navio + 0.008*flor + 0.008*cam\\xc3\\xb5es',\n",
        " '0.068*senhora + 0.050*lu\\xc3\\xads + 0.034*carlota + 0.030*oliver + 0.030*senhor + 0.027*gente + 0.021*pessoa + 0.020*desde + 0.016*cabe\\xc3\\xa7a + 0.015*horas',\n",
        " '0.031*judeu + 0.023*raz\\xc3\\xa3o + 0.022*jorge + 0.022*quer + 0.021*modo + 0.021*devia + 0.020*ponto + 0.020*nenhuma + 0.017*nenhum + 0.017*mundo',\n",
        " '0.032*lado + 0.024*preciso + 0.023*sabe + 0.021*sala + 0.021*primeira + 0.019*terra + 0.019*lugar + 0.018*parte + 0.017*parece + 0.015*quero',\n",
        " '0.046*bar\\xc3\\xa3o + 0.046*cena + 0.037*dumont + 0.032*joana + 0.027*mathilde + 0.026*alvarez + 0.023*rosinha + 0.021*durval + 0.019*tito + 0.018*larcey',\n",
        " '0.013*respondeu + 0.011*parecia + 0.007*pobre + 0.007*continuou + 0.007*entrou + 0.007*ficou + 0.006*fazia + 0.006*queria + 0.006*crian\\xc3\\xa7a + 0.006*velha']"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "glob.glob('./machado/tokens/roman*')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "['./machado/tokens\\\\romance_marm01.txt',\n",
        " './machado/tokens\\\\romance_marm02.txt',\n",
        " './machado/tokens\\\\romance_marm03.txt',\n",
        " './machado/tokens\\\\romance_marm04.txt',\n",
        " './machado/tokens\\\\romance_marm05.txt',\n",
        " './machado/tokens\\\\romance_marm06.txt',\n",
        " './machado/tokens\\\\romance_marm07.txt',\n",
        " './machado/tokens\\\\romance_marm08.txt',\n",
        " './machado/tokens\\\\romance_marm09.txt',\n",
        " './machado/tokens\\\\romance_marm10.txt']"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clear%"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}