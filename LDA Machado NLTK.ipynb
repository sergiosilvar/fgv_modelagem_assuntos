{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### M\u00e9todos para pre processamento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import os\n",
      "import numpy as np\n",
      "import datetime\n",
      "\n",
      "from nltk.corpus import machado as mch\n",
      "import nltk\n",
      "from gensim import corpora, models, similarities\n",
      "from gensim.models import hdpmodel, ldamodel\n",
      "\n",
      "\n",
      "reload(sys)\n",
      "sys.setdefaultencoding(\"utf-8\")\n",
      "\n",
      "def nomes_documentos():\n",
      "    return os.listdir('./machado/tokens')\n",
      "\n",
      "def stopwords():\n",
      "    f = open('stopwords_pt.txt','r')\n",
      "    stopw = f.read().split('\\n')\n",
      "    f.close()\n",
      "    return stopw\n",
      "\n",
      "def cria_pastas():\n",
      "    dirs = ['./machado/tokens','./machado/freq','./machado/vocab']\n",
      "    for d in dirs:\n",
      "        if not os.path.exists(d):\n",
      "            os.makedirs(d)\n",
      "\n",
      "def pre_processamento():\n",
      "    \n",
      "    for fileid in mch.fileids():\n",
      "        t1 = datetime.datetime.now()\n",
      "        print 'Tokenizar %s...' % fileid \n",
      "        \n",
      "        # Carrega todas as palavras.\n",
      "        words = mch.words(fileid)\n",
      "        \n",
      "        # Filtra palavras menores que 3 caracteres e stopwords.\n",
      "        doc = [w.lower() for w in words if len(w) > 3 and  w.lower() not in stopwords()]\n",
      "\n",
      "        # Filtra palavras que aparecem menos de 10 vezes.\n",
      "        fd = nltk.FreqDist(doc)\n",
      "        doc = [w for w in doc if fd[w] > 10]\n",
      "\n",
      "        # Salva palavras em um novo documentos.\n",
      "        f = open('./machado/tokens/'+fileid.replace('/','_'),'w')\n",
      "        for w in doc:\n",
      "            f.write(w+u'\\n')\n",
      "        f.close()\n",
      "        delta_t = datetime.datetime.now()-t1\n",
      "        print 'Tokenizado  ''%s'' em %d segundos.'%(fileid, delta_t.seconds) \n",
      "\n",
      "        \n",
      "def registra_freq():\n",
      "    for i in nomes_documentos():\n",
      "        fdoc = open('./machado/tokens/' + i,'r')\n",
      "        doc = fdoc.read().split('\\n')\n",
      "        fdoc.close()\n",
      "        fd = nltk.FreqDist(doc)\n",
      "        ffreq = open('./machado/freq/'+i, 'w')\n",
      "        for k,v in fd.items():\n",
      "            ffreq.write(k+';'+str(v)+'\\n')\n",
      "        ffreq.close()\n",
      "        \n",
      "        \n",
      "\n",
      "def constroi_vocabulario():\n",
      "    items = None\n",
      "    vocab = set([])\n",
      "    palavras = []\n",
      "    for arqv in os.listdir('./machado/freq'):\n",
      "        f = open('./machado/freq/'+arqv, 'r')\n",
      "        items = f.read().split('\\n')\n",
      "        f.close()\n",
      "        lista = []\n",
      "        for i in items:\n",
      "            word = i.split(';')[0]\n",
      "            lista.append(word)\n",
      "            palavras.append(word)\n",
      "        vocab = set(list(vocab)+lista)\n",
      "    f = open('./machado/vocab/vocab.txt','w')\n",
      "    txt = '\\n'.join(sorted(vocab))\n",
      "    f.write(txt)\n",
      "    f.close()\n",
      "\n",
      "    # Registra a ocorr\u00eancia das palavras entre os documentos.\n",
      "    fd = nltk.FreqDist(palavras)\n",
      "    ffreq = open('./machado/palavras_comuns.txt','w')\n",
      "    for k,v in fd.items():\n",
      "        ffreq.write(k+';'+str(v)+'\\n')\n",
      "    ffreq.close()\n",
      "\n",
      "def palavras_comuns():    \n",
      "    f = open('./machado/palavras_comuns.txt', 'r')\n",
      "    txt = f.read().splitlines()\n",
      "    f.close()\n",
      "    return txt\n",
      "\n",
      "\n",
      "def vocabulario():\n",
      "    f = open('./machado/vocab/vocab.txt', 'r')\n",
      "    txt = f.read().splitlines()\n",
      "    f.close()\n",
      "    return txt\n",
      "\n",
      "def documentos():\n",
      "    arqvs = os.listdir('./machado/tokens')\n",
      "    docs = []\n",
      "    for arq in arqvs:\n",
      "        f = open('./machado/tokens/'+arq)\n",
      "        words = f.read().splitlines()\n",
      "        #words = ' '.join(words)\n",
      "        docs.append(words)\n",
      "        f.close()\n",
      "    return docs\n",
      "\n",
      "\n",
      "def lda_flavio(num_topics=10,pasta='./'):\n",
      "    vocab = vocabulario()\n",
      "    docset = documentos()\n",
      "    \n",
      "    K=num_topics\n",
      "    D = len(docset) #Number of documents in the docset\n",
      "    olda = onlineldavb.OnlineLDA(vocab, K, D, 1./K, 1./K, 1024, 0.7)\n",
      "    i = 0\n",
      "    for doc in docset:\n",
      "        t1 = datetime.datetime.now()\n",
      "        i += 1\n",
      "        print 'Analistando documento %d de %d.' % (i, D)\n",
      "        gamma, bound = olda.update_lambda(doc)\n",
      "        wordids, wordcts = onlineldavb.parse_doc_list(doc,olda._vocab)\n",
      "        perwordbound = bound * len(docset) / (D*sum(map(sum,wordcts)))\n",
      "        delta_t = datetime.datetime.now()-t1\n",
      "        print 'Documento  %d de %d em %d segundos.'%(i, D, delta_t.seconds) #2134 segundos ~ 35 min.\n",
      "\n",
      "    np.savetxt(pasta+'lda_lambda.txt',olda._lambda)\n",
      "    print 'Fim do LDA'\n",
      "    return vocab, olda._lambda\n",
      "\n",
      "\n",
      "def lda_gensim(num_topics):\n",
      "    texts = documentos()\n",
      "    \n",
      "    dictionary = corpora.Dictionary(texts)\n",
      "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "    \n",
      "    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
      "    \n",
      "    lda.save('./machado/machado.lda')\n",
      "    dictionary.save('./machado/machado.dic')\n",
      "    dictionary.save_as_text('./machado/machado_dic.txt')  \n",
      "    \n",
      "    \n",
      "    return (dictionary,corpus,lda)\n",
      "\n",
      "def gensim_extract(lda, n_topics=10, n_words=10):\n",
      "    p = []\n",
      "    for top in lda.show_topics(topics=n_topics, topn=n_words):\n",
      "        lista_p = top.split(' + ') \n",
      "        vk = [(l.split('*')[1],float(l.split('*')[0])) for l in lista_p ]\n",
      "        p.append(vk)\n",
      "        #[l[1] for l in lista_v]\n",
      "    return p\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Visualiza\u00e7\u00e3o das palavras.\n",
      "\n",
      "Baseado em https://de.dariah.eu/tatom/topic_model_visualization.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pre processamento, usar uma \u00fanica vez."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pre_processamento()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "registra_freq()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "constroi_vocabulario()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dic,corpus, lda = lda_gensim(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Carregar LDA feito anteriomente"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lda = ldamodel.LdaModel.load('./machado/machado.lda')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## An\u00e1lise do corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gensim_extract(lda, n_words=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[[('casa', 0.01),\n",
        "  ('dois', 0.009),\n",
        "  ('coisa', 0.009),\n",
        "  ('olhos', 0.009),\n",
        "  ('tempo', 0.008),\n",
        "  ('mo\\xc3\\xa7a', 0.008),\n",
        "  ('pode', 0.007),\n",
        "  ('todos', 0.007),\n",
        "  ('dias', 0.006),\n",
        "  ('homem', 0.006),\n",
        "  ('duas', 0.006),\n",
        "  ('alguma', 0.006),\n",
        "  ('anos', 0.006),\n",
        "  ('vida', 0.005),\n",
        "  ('podia', 0.005),\n",
        "  ('outros', 0.005),\n",
        "  ('grande', 0.005),\n",
        "  ('alguns', 0.005),\n",
        "  ('dizer', 0.005),\n",
        "  ('fazer', 0.005)],\n",
        " [('coisa', 0.01),\n",
        "  ('tempo', 0.01),\n",
        "  ('dois', 0.009),\n",
        "  ('olhos', 0.008),\n",
        "  ('casa', 0.008),\n",
        "  ('todos', 0.008),\n",
        "  ('homem', 0.007),\n",
        "  ('pode', 0.006),\n",
        "  ('amor', 0.006),\n",
        "  ('podia', 0.006),\n",
        "  ('dias', 0.006),\n",
        "  ('dizer', 0.006),\n",
        "  ('alguns', 0.005),\n",
        "  ('grande', 0.005),\n",
        "  ('alguma', 0.005),\n",
        "  ('vida', 0.005),\n",
        "  ('mo\\xc3\\xa7a', 0.005),\n",
        "  ('anos', 0.005),\n",
        "  ('duas', 0.005),\n",
        "  ('mesma', 0.004)],\n",
        " [('casa', 0.015),\n",
        "  ('olhos', 0.011),\n",
        "  ('coisa', 0.01),\n",
        "  ('homem', 0.008),\n",
        "  ('tempo', 0.008),\n",
        "  ('dois', 0.008),\n",
        "  ('vida', 0.007),\n",
        "  ('duas', 0.007),\n",
        "  ('mo\\xc3\\xa7a', 0.007),\n",
        "  ('dizer', 0.006),\n",
        "  ('todos', 0.006),\n",
        "  ('amor', 0.006),\n",
        "  ('alguma', 0.006),\n",
        "  ('meneses', 0.006),\n",
        "  ('pode', 0.005),\n",
        "  ('mulher', 0.005),\n",
        "  ('grande', 0.005),\n",
        "  ('anos', 0.005),\n",
        "  ('fazer', 0.005),\n",
        "  ('noite', 0.005)],\n",
        " [('tempo', 0.011),\n",
        "  ('coisa', 0.008),\n",
        "  ('olhos', 0.008),\n",
        "  ('dois', 0.007),\n",
        "  ('homem', 0.007),\n",
        "  ('casa', 0.007),\n",
        "  ('todos', 0.006),\n",
        "  ('duas', 0.006),\n",
        "  ('dizer', 0.006),\n",
        "  ('pode', 0.006),\n",
        "  ('dias', 0.006),\n",
        "  ('alguns', 0.006),\n",
        "  ('verdade', 0.006),\n",
        "  ('podia', 0.005),\n",
        "  ('vida', 0.005),\n",
        "  ('anos', 0.005),\n",
        "  ('grande', 0.005),\n",
        "  ('padre', 0.005),\n",
        "  ('fazer', 0.005),\n",
        "  ('mesma', 0.004)],\n",
        " [('casa', 0.01),\n",
        "  ('tempo', 0.008),\n",
        "  ('coisa', 0.008),\n",
        "  ('olhos', 0.007),\n",
        "  ('cora\\xc3\\xa7\\xc3\\xa3o', 0.007),\n",
        "  ('alguns', 0.006),\n",
        "  ('dois', 0.006),\n",
        "  ('todos', 0.006),\n",
        "  ('dias', 0.005),\n",
        "  ('amor', 0.005),\n",
        "  ('homem', 0.005),\n",
        "  ('pode', 0.005),\n",
        "  ('alguma', 0.005),\n",
        "  ('duas', 0.005),\n",
        "  ('grande', 0.005),\n",
        "  ('outros', 0.005),\n",
        "  ('mo\\xc3\\xa7a', 0.005),\n",
        "  ('fazer', 0.005),\n",
        "  ('melhor', 0.005),\n",
        "  ('sobre', 0.004)],\n",
        " [('casa', 0.019),\n",
        "  ('tempo', 0.014),\n",
        "  ('coisa', 0.011),\n",
        "  ('olhos', 0.01),\n",
        "  ('homem', 0.009),\n",
        "  ('dois', 0.009),\n",
        "  ('vida', 0.008),\n",
        "  ('mo\\xc3\\xa7a', 0.007),\n",
        "  ('dizer', 0.007),\n",
        "  ('todos', 0.007),\n",
        "  ('duas', 0.006),\n",
        "  ('pode', 0.006),\n",
        "  ('anos', 0.006),\n",
        "  ('grande', 0.006),\n",
        "  ('helena', 0.006),\n",
        "  ('alguns', 0.005),\n",
        "  ('amigo', 0.005),\n",
        "  ('cora\\xc3\\xa7\\xc3\\xa3o', 0.005),\n",
        "  ('podia', 0.005),\n",
        "  ('dias', 0.005)],\n",
        " [('casa', 0.014),\n",
        "  ('tempo', 0.011),\n",
        "  ('coisa', 0.009),\n",
        "  ('vida', 0.009),\n",
        "  ('amor', 0.009),\n",
        "  ('olhos', 0.008),\n",
        "  ('mo\\xc3\\xa7a', 0.008),\n",
        "  ('cora\\xc3\\xa7\\xc3\\xa3o', 0.007),\n",
        "  ('homem', 0.007),\n",
        "  ('noite', 0.007),\n",
        "  ('poeta', 0.007),\n",
        "  ('dizer', 0.007),\n",
        "  ('todos', 0.006),\n",
        "  ('dois', 0.005),\n",
        "  ('maria', 0.005),\n",
        "  ('outros', 0.005),\n",
        "  ('pode', 0.005),\n",
        "  ('carta', 0.005),\n",
        "  ('dias', 0.005),\n",
        "  ('vi\\xc3\\xbava', 0.005)],\n",
        " [('casa', 0.01),\n",
        "  ('tempo', 0.01),\n",
        "  ('olhos', 0.009),\n",
        "  ('dois', 0.009),\n",
        "  ('coisa', 0.008),\n",
        "  ('homem', 0.008),\n",
        "  ('alguma', 0.007),\n",
        "  ('amor', 0.007),\n",
        "  ('cora\\xc3\\xa7\\xc3\\xa3o', 0.007),\n",
        "  ('duas', 0.006),\n",
        "  ('lu\\xc3\\xads', 0.006),\n",
        "  ('alguns', 0.006),\n",
        "  ('noite', 0.006),\n",
        "  ('dizer', 0.006),\n",
        "  ('grande', 0.006),\n",
        "  ('mo\\xc3\\xa7a', 0.006),\n",
        "  ('podia', 0.006),\n",
        "  ('alma', 0.005),\n",
        "  ('senhor', 0.005),\n",
        "  ('todos', 0.005)],\n",
        " [('tempo', 0.008),\n",
        "  ('todos', 0.008),\n",
        "  ('dois', 0.008),\n",
        "  ('coisa', 0.006),\n",
        "  ('pode', 0.006),\n",
        "  ('homem', 0.006),\n",
        "  ('vida', 0.006),\n",
        "  ('olhos', 0.006),\n",
        "  ('casa', 0.006),\n",
        "  ('alguma', 0.006),\n",
        "  ('grande', 0.006),\n",
        "  ('outros', 0.006),\n",
        "  ('dias', 0.005),\n",
        "  ('duas', 0.005),\n",
        "  ('dizer', 0.005),\n",
        "  ('alguns', 0.005),\n",
        "  ('verdade', 0.004),\n",
        "  ('sobre', 0.004),\n",
        "  ('algum', 0.004),\n",
        "  ('anos', 0.004)],\n",
        " [('casa', 0.017),\n",
        "  ('olhos', 0.009),\n",
        "  ('dois', 0.009),\n",
        "  ('tempo', 0.009),\n",
        "  ('coisa', 0.008),\n",
        "  ('todos', 0.008),\n",
        "  ('duas', 0.007),\n",
        "  ('dizer', 0.007),\n",
        "  ('amor', 0.007),\n",
        "  ('pode', 0.006),\n",
        "  ('dias', 0.006),\n",
        "  ('tito', 0.006),\n",
        "  ('homem', 0.006),\n",
        "  ('jo\\xc3\\xa3o', 0.006),\n",
        "  ('em\\xc3\\xadlia', 0.006),\n",
        "  ('vida', 0.005),\n",
        "  ('alguma', 0.005),\n",
        "  ('vezes', 0.005),\n",
        "  ('anos', 0.005),\n",
        "  ('podia', 0.005)]]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### LDA NAMD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab,lambd = lda_flavio(pasta='./machado/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from   Topics.visualization.topiccloud import GenCloud\n",
      "\n",
      "K=10\n",
      "cloud = GenCloud(vocab,lambd)\n",
      "for i in range(K):\n",
      "  cloud.gen_image(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "cannot convert float NaN to integer",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-15-1357b827a844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m   \u001b[0mcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Users\\sergio\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\topics-0.2.5-py2.7-win-amd64.egg\\Topics\\visualization\\topiccloud.pyc\u001b[0m in \u001b[0;36mgen_image\u001b[1;34m(self, topic, fname, width, height)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInteger\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mline\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \"\"\"\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mmake_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"{}_{}.png\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sergio\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\topics-0.2.5-py2.7-win-amd64.egg\\Topics\\visualization\\wordcloud.pyc\u001b[0m in \u001b[0;36mmake_wordcloud\u001b[1;34m(words, counts, fname, font_path, width, height, margin, ranks_only)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# alternative way to set the font size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mranks_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mfont_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# try to find a position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vocab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "3674"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}